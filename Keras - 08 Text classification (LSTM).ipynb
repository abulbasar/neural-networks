{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout, Embedding, Conv1D, MaxPooling1D, Flatten\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i went and saw this movie last night after bei...</td>\n",
       "      <td>test</td>\n",
       "      <td>0_10.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actor turned director bill paxton follows up h...</td>\n",
       "      <td>test</td>\n",
       "      <td>10000_7.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as a recreational golfer with some knowledge o...</td>\n",
       "      <td>test</td>\n",
       "      <td>10001_9.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i saw this film in a sneak preview and it is d...</td>\n",
       "      <td>test</td>\n",
       "      <td>10002_8.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bill paxton has taken the true story of the 19...</td>\n",
       "      <td>test</td>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content label         name  \\\n",
       "0  i went and saw this movie last night after bei...  test     0_10.txt   \n",
       "1  actor turned director bill paxton follows up h...  test  10000_7.txt   \n",
       "2  as a recreational golfer with some knowledge o...  test  10001_9.txt   \n",
       "3  i saw this film in a sneak preview and it is d...  test  10002_8.txt   \n",
       "4  bill paxton has taken the true story of the 19...  test  10003_8.txt   \n",
       "\n",
       "  sentiment  \n",
       "0       pos  \n",
       "1       pos  \n",
       "2       pos  \n",
       "3       pos  \n",
       "4       pos  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_imdb(path):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re, json\n",
    "    import pandas as pd\n",
    "    \n",
    "    def preprocess(text):\n",
    "        text = BeautifulSoup(text.lower(), \"html5lib\").text #removed html tags\n",
    "        text = re.sub(r\"[\\W]+\", \" \", text)\n",
    "        return text\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        comments = pd.DataFrame.from_dict([json.loads(l) for l in f])\n",
    "        comments[\"content\"] = comments[\"content\"].apply(preprocess)\n",
    "        return comments\n",
    "        \n",
    "comments = load_imdb(\"/data/imdb-comments.json\")\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46875</th>\n",
       "      <td>what an utter disappointment the score of 6 1 ...</td>\n",
       "      <td>train</td>\n",
       "      <td>7189_4.txt</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14363</th>\n",
       "      <td>the movie uses random events of historical sig...</td>\n",
       "      <td>test</td>\n",
       "      <td>11678_2.txt</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48591</th>\n",
       "      <td>possible plot spoilers i adore dennis hopper ...</td>\n",
       "      <td>train</td>\n",
       "      <td>8733_1.txt</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18123</th>\n",
       "      <td>i first heard of this movie at the flashback w...</td>\n",
       "      <td>test</td>\n",
       "      <td>3811_4.txt</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>this is a good movie i won t go into any detai...</td>\n",
       "      <td>test</td>\n",
       "      <td>7852_8.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11553</th>\n",
       "      <td>this film is a good start for novices that hav...</td>\n",
       "      <td>test</td>\n",
       "      <td>9149_8.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10802</th>\n",
       "      <td>a masterful performance by jamie foxx is just ...</td>\n",
       "      <td>test</td>\n",
       "      <td>8473_9.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>an obsessive love story where the characters h...</td>\n",
       "      <td>test</td>\n",
       "      <td>10284_9.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39764</th>\n",
       "      <td>does anything at all happen in this movie ther...</td>\n",
       "      <td>train</td>\n",
       "      <td>12038_4.txt</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>ok the show was a little uneven but i still lo...</td>\n",
       "      <td>test</td>\n",
       "      <td>11569_7.txt</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label         name  \\\n",
       "46875  what an utter disappointment the score of 6 1 ...  train   7189_4.txt   \n",
       "14363  the movie uses random events of historical sig...   test  11678_2.txt   \n",
       "48591   possible plot spoilers i adore dennis hopper ...  train   8733_1.txt   \n",
       "18123  i first heard of this movie at the flashback w...   test   3811_4.txt   \n",
       "10112  this is a good movie i won t go into any detai...   test   7852_8.txt   \n",
       "11553  this film is a good start for novices that hav...   test   9149_8.txt   \n",
       "10802  a masterful performance by jamie foxx is just ...   test   8473_9.txt   \n",
       "315    an obsessive love story where the characters h...   test  10284_9.txt   \n",
       "39764  does anything at all happen in this movie ther...  train  12038_4.txt   \n",
       "1742   ok the show was a little uneven but i still lo...   test  11569_7.txt   \n",
       "\n",
       "      sentiment  \n",
       "46875       neg  \n",
       "14363       neg  \n",
       "48591       neg  \n",
       "18123       neg  \n",
       "10112       pos  \n",
       "11553       pos  \n",
       "10802       pos  \n",
       "315         pos  \n",
       "39764       neg  \n",
       "1742        pos  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(comments.sentiment == \"pos\", 1, 0)\n",
    "is_training = comments.label == \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(comments.content)\n",
    "doc_terms = tokenizer.texts_to_sequences(comments.content)\n",
    "len(doc_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i went and saw this movie last night after being coaxed to by a few friends of mine i ll admit that i was reluctant to see it because from what i knew of ashton kutcher he was only able to do comedy i was wrong kutcher played the character of jake fischer very well and kevin costner played ben randall with such professionalism the sign of a good movie is that it can toy with our emotions this one did exactly that the entire theater which was sold out was overcome by laughter during the first half of the movie and were moved to tears during the second half while exiting the theater i not only saw many women in tears but many full grown men as well trying desperately not to let anyone see them crying this movie was great and i suggest that you go see it before you judge '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 416, 2, 210, 10, 15, 238, 311, 100, 109, 28203, 5, 33, 3, 173, 352, 4, 1758, 9, 232, 975, 11, 9, 13, 5735, 5, 65, 7, 84, 37, 47, 9, 672, 4, 9942, 9315, 24, 13, 62, 477, 5, 78, 201, 9, 13, 356, 9315, 254, 1, 104, 4, 3373, 15901, 53, 69, 2, 1621, 7132, 254, 1155, 8399, 17, 139, 11306, 1, 2016, 4, 3, 49, 15, 6, 11, 7, 50, 2970, 17, 258, 1334, 10, 28, 117, 619, 11, 1, 442, 764, 61, 13, 2986, 43, 13, 3232, 33, 2142, 303, 1, 88, 304, 4, 1, 15, 2, 71, 1649, 5, 1716, 303, 1, 338, 304, 136, 11778, 1, 764, 9, 23, 62, 210, 107, 362, 8, 1716, 18, 107, 364, 2061, 339, 14, 69, 264, 2816, 23, 5, 276, 246, 65, 92, 2488, 10, 15, 13, 80, 2, 9, 1466, 11, 20, 141, 65, 7, 161, 20, 1671]\n"
     ]
    }
   ],
   "source": [
    "print(doc_terms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words_by_index = dict([(i, word) for word, i in tokenizer.word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'went', 'and', 'saw', 'this', 'movie', 'last', 'night', 'after', 'being', 'coaxed', 'to', 'by', 'a', 'few', 'friends', 'of', 'mine', 'i', 'll', 'admit', 'that', 'i', 'was', 'reluctant', 'to', 'see', 'it', 'because', 'from', 'what', 'i', 'knew', 'of', 'ashton', 'kutcher', 'he', 'was', 'only', 'able', 'to', 'do', 'comedy', 'i', 'was', 'wrong', 'kutcher', 'played', 'the', 'character', 'of', 'jake', 'fischer', 'very', 'well', 'and', 'kevin', 'costner', 'played', 'ben', 'randall', 'with', 'such', 'professionalism', 'the', 'sign', 'of', 'a', 'good', 'movie', 'is', 'that', 'it', 'can', 'toy', 'with', 'our', 'emotions', 'this', 'one', 'did', 'exactly', 'that', 'the', 'entire', 'theater', 'which', 'was', 'sold', 'out', 'was', 'overcome', 'by', 'laughter', 'during', 'the', 'first', 'half', 'of', 'the', 'movie', 'and', 'were', 'moved', 'to', 'tears', 'during', 'the', 'second', 'half', 'while', 'exiting', 'the', 'theater', 'i', 'not', 'only', 'saw', 'many', 'women', 'in', 'tears', 'but', 'many', 'full', 'grown', 'men', 'as', 'well', 'trying', 'desperately', 'not', 'to', 'let', 'anyone', 'see', 'them', 'crying', 'this', 'movie', 'was', 'great', 'and', 'i', 'suggest', 'that', 'you', 'go', 'see', 'it', 'before', 'you', 'judge']\n"
     ]
    }
   ],
   "source": [
    "print([words_by_index[t] for t in doc_terms[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103890"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4d7e069f28>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series([len(r) for r in doc_terms]).plot.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     9,   416,     2,   210,    10,    15,   238,   311,\n",
       "         100,   109, 28203,     5,    33,     3,   173,   352,     4,\n",
       "        1758,     9,   232,   975,    11,     9,    13,  5735,     5,\n",
       "          65,     7,    84,    37,    47,     9,   672,     4,  9942,\n",
       "        9315,    24,    13,    62,   477,     5,    78,   201,     9,\n",
       "          13,   356,  9315,   254,     1,   104,     4,  3373, 15901,\n",
       "          53,    69,     2,  1621,  7132,   254,  1155,  8399,    17,\n",
       "         139, 11306,     1,  2016,     4,     3,    49,    15,     6,\n",
       "          11,     7,    50,  2970,    17,   258,  1334,    10,    28,\n",
       "         117,   619,    11,     1,   442,   764,    61,    13,  2986,\n",
       "          43,    13,  3232,    33,  2142,   303,     1,    88,   304,\n",
       "           4,     1,    15,     2,    71,  1649,     5,  1716,   303,\n",
       "           1,   338,   304,   136, 11778,     1,   764,     9,    23,\n",
       "          62,   210,   107,   362,     8,  1716,    18,   107,   364,\n",
       "        2061,   339,    14,    69,   264,  2816,    23,     5,   276,\n",
       "         246,    65,    92,  2488,    10,    15,    13,    80,     2,\n",
       "           9,  1466,    11,    20,   141,    65,     7,   161,    20,\n",
       "        1671], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 1000\n",
    "doc_terms_padded = pad_sequences(doc_terms, maxlen=maxlen)\n",
    "doc_terms_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 1000, 10)          10010     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1000, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 996, 64)           3264      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 247, 128)          24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 61, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 400)               51600     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 221,563\n",
      "Trainable params: 221,563\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.5383 - acc: 0.7220 - val_loss: 0.5674 - val_acc: 0.7222\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4357 - acc: 0.7989 - val_loss: 0.4417 - val_acc: 0.7952\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3981 - acc: 0.8207 - val_loss: 0.3798 - val_acc: 0.8375\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3766 - acc: 0.8332 - val_loss: 0.3682 - val_acc: 0.8369\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3637 - acc: 0.8396 - val_loss: 0.3776 - val_acc: 0.8393\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3559 - acc: 0.8432 - val_loss: 0.3841 - val_acc: 0.8258\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3455 - acc: 0.8490 - val_loss: 0.3552 - val_acc: 0.8407\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3398 - acc: 0.8520 - val_loss: 0.4306 - val_acc: 0.8106\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3353 - acc: 0.8551 - val_loss: 0.3669 - val_acc: 0.8364\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3323 - acc: 0.8545 - val_loss: 0.4023 - val_acc: 0.8282\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3283 - acc: 0.8571 - val_loss: 0.3530 - val_acc: 0.8473\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3239 - acc: 0.8601 - val_loss: 0.3463 - val_acc: 0.8484\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3229 - acc: 0.8619 - val_loss: 0.3553 - val_acc: 0.8474\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3170 - acc: 0.8643 - val_loss: 0.3347 - val_acc: 0.8551\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3180 - acc: 0.8626 - val_loss: 0.3354 - val_acc: 0.8542\n",
      "Epoch 16/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3144 - acc: 0.8654 - val_loss: 0.3456 - val_acc: 0.8520\n",
      "Epoch 17/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3126 - acc: 0.8654 - val_loss: 0.3317 - val_acc: 0.8542\n",
      "Epoch 18/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3098 - acc: 0.8664 - val_loss: 0.3471 - val_acc: 0.8492\n",
      "Epoch 19/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3105 - acc: 0.8654 - val_loss: 0.3568 - val_acc: 0.8480\n",
      "Epoch 20/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3068 - acc: 0.8669 - val_loss: 0.3508 - val_acc: 0.8490\n",
      "Epoch 21/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3081 - acc: 0.8665 - val_loss: 0.3750 - val_acc: 0.8367\n",
      "Epoch 22/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3057 - acc: 0.8702 - val_loss: 0.3460 - val_acc: 0.8470\n",
      "Epoch 23/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3042 - acc: 0.8709 - val_loss: 0.3401 - val_acc: 0.8521\n",
      "Epoch 24/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3044 - acc: 0.8682 - val_loss: 0.3367 - val_acc: 0.8540\n",
      "Epoch 25/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3011 - acc: 0.8722 - val_loss: 0.3530 - val_acc: 0.8500\n",
      "Epoch 26/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3007 - acc: 0.8699 - val_loss: 0.3344 - val_acc: 0.8523\n",
      "Epoch 27/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3007 - acc: 0.8717 - val_loss: 0.3561 - val_acc: 0.8490\n",
      "Epoch 28/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2996 - acc: 0.8710 - val_loss: 0.3473 - val_acc: 0.8521\n",
      "Epoch 29/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2999 - acc: 0.8715 - val_loss: 0.3407 - val_acc: 0.8519\n",
      "Epoch 30/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2970 - acc: 0.8734 - val_loss: 0.3410 - val_acc: 0.8530\n",
      "Epoch 31/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2977 - acc: 0.8728 - val_loss: 0.3666 - val_acc: 0.8385\n",
      "Epoch 32/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2933 - acc: 0.8756 - val_loss: 0.3465 - val_acc: 0.8492\n",
      "Epoch 33/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2956 - acc: 0.8717 - val_loss: 0.3837 - val_acc: 0.8319\n",
      "Epoch 34/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2968 - acc: 0.8704 - val_loss: 0.3486 - val_acc: 0.8531\n",
      "Epoch 35/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2943 - acc: 0.8720 - val_loss: 0.3520 - val_acc: 0.8466\n",
      "Epoch 36/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2885 - acc: 0.8772 - val_loss: 0.3552 - val_acc: 0.8525\n",
      "Epoch 37/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2888 - acc: 0.8768 - val_loss: 0.3511 - val_acc: 0.8484\n",
      "Epoch 38/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2887 - acc: 0.8750 - val_loss: 0.3653 - val_acc: 0.8508\n",
      "Epoch 39/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2864 - acc: 0.8778 - val_loss: 0.3668 - val_acc: 0.8448\n",
      "Epoch 40/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2866 - acc: 0.8777 - val_loss: 0.3481 - val_acc: 0.8486\n",
      "Epoch 41/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2863 - acc: 0.8784 - val_loss: 0.3878 - val_acc: 0.8408\n",
      "Epoch 42/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2882 - acc: 0.8757 - val_loss: 0.3539 - val_acc: 0.8465\n",
      "Epoch 43/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2874 - acc: 0.8782 - val_loss: 0.3463 - val_acc: 0.8504\n",
      "Epoch 44/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2890 - acc: 0.8761 - val_loss: 0.3566 - val_acc: 0.8464\n",
      "Epoch 45/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2877 - acc: 0.8745 - val_loss: 0.3516 - val_acc: 0.8499\n",
      "Epoch 46/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2853 - acc: 0.8767 - val_loss: 0.3437 - val_acc: 0.8526\n",
      "Epoch 47/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2868 - acc: 0.8766 - val_loss: 0.3506 - val_acc: 0.8492\n",
      "Epoch 48/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2822 - acc: 0.8786 - val_loss: 0.3716 - val_acc: 0.8444\n",
      "Epoch 49/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2855 - acc: 0.8779 - val_loss: 0.3565 - val_acc: 0.8447\n",
      "Epoch 50/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2831 - acc: 0.8779 - val_loss: 0.3643 - val_acc: 0.8450\n",
      "Epoch 51/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2851 - acc: 0.8771 - val_loss: 0.3571 - val_acc: 0.8460\n",
      "Epoch 52/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2891 - acc: 0.8736 - val_loss: 0.3660 - val_acc: 0.8468\n",
      "Epoch 53/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2848 - acc: 0.8784 - val_loss: 0.3521 - val_acc: 0.8458\n",
      "Epoch 54/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2862 - acc: 0.8764 - val_loss: 0.3492 - val_acc: 0.8470\n",
      "Epoch 55/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2806 - acc: 0.8778 - val_loss: 0.3698 - val_acc: 0.8484\n",
      "Epoch 56/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2837 - acc: 0.8772 - val_loss: 0.3573 - val_acc: 0.8487\n",
      "Epoch 57/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2850 - acc: 0.8764 - val_loss: 0.3631 - val_acc: 0.8468\n",
      "Epoch 58/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2897 - acc: 0.8751 - val_loss: 0.3551 - val_acc: 0.8450\n",
      "Epoch 59/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2880 - acc: 0.8759 - val_loss: 0.3676 - val_acc: 0.8422\n",
      "Epoch 60/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2849 - acc: 0.8777 - val_loss: 0.3682 - val_acc: 0.8470\n",
      "Epoch 61/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2794 - acc: 0.8808 - val_loss: 0.3543 - val_acc: 0.8459\n",
      "Epoch 62/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2792 - acc: 0.8786 - val_loss: 0.3768 - val_acc: 0.8478\n",
      "Epoch 63/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2816 - acc: 0.8782 - val_loss: 0.3825 - val_acc: 0.8453\n",
      "Epoch 64/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2825 - acc: 0.8786 - val_loss: 0.3643 - val_acc: 0.8460\n",
      "Epoch 65/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2786 - acc: 0.8807 - val_loss: 0.3633 - val_acc: 0.8470\n",
      "Epoch 66/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2790 - acc: 0.8799 - val_loss: 0.3764 - val_acc: 0.8423\n",
      "Epoch 67/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2800 - acc: 0.8794 - val_loss: 0.3678 - val_acc: 0.8484\n",
      "Epoch 68/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2777 - acc: 0.8780 - val_loss: 0.3762 - val_acc: 0.8396\n",
      "Epoch 69/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2770 - acc: 0.8798 - val_loss: 0.3574 - val_acc: 0.8478\n",
      "Epoch 70/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2762 - acc: 0.8809 - val_loss: 0.3987 - val_acc: 0.8438\n",
      "Epoch 71/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2800 - acc: 0.8798 - val_loss: 0.3667 - val_acc: 0.8470\n",
      "Epoch 72/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2767 - acc: 0.8793 - val_loss: 0.3773 - val_acc: 0.8405\n",
      "Epoch 73/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2712 - acc: 0.8841 - val_loss: 0.3591 - val_acc: 0.8453\n",
      "Epoch 74/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2766 - acc: 0.8819 - val_loss: 0.3683 - val_acc: 0.8462\n",
      "Epoch 75/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2762 - acc: 0.8837 - val_loss: 0.3700 - val_acc: 0.8438\n",
      "Epoch 76/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2759 - acc: 0.8800 - val_loss: 0.3787 - val_acc: 0.8429\n",
      "Epoch 77/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2723 - acc: 0.8850 - val_loss: 0.3797 - val_acc: 0.8420\n",
      "Epoch 78/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2772 - acc: 0.8816 - val_loss: 0.3709 - val_acc: 0.8426\n",
      "Epoch 79/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2819 - acc: 0.8768 - val_loss: 0.3829 - val_acc: 0.8439\n",
      "Epoch 80/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2779 - acc: 0.8788 - val_loss: 0.3735 - val_acc: 0.8422\n",
      "Epoch 81/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2885 - acc: 0.8716 - val_loss: 0.3656 - val_acc: 0.8414\n",
      "Epoch 82/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2820 - acc: 0.8775 - val_loss: 0.3705 - val_acc: 0.8432\n",
      "Epoch 83/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2804 - acc: 0.8782 - val_loss: 0.3728 - val_acc: 0.8384\n",
      "Epoch 84/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2780 - acc: 0.8814 - val_loss: 0.3772 - val_acc: 0.8423\n",
      "Epoch 85/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2780 - acc: 0.8800 - val_loss: 0.3684 - val_acc: 0.8416\n",
      "Epoch 86/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2748 - acc: 0.8826 - val_loss: 0.3758 - val_acc: 0.8412\n",
      "Epoch 87/100\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2804 - acc: 0.8782 - val_loss: 0.3962 - val_acc: 0.8380\n",
      "Epoch 88/100\n",
      " 3904/25000 [===>..........................] - ETA: 28s - loss: 0.2840 - acc: 0.8719"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(maxlen + 1, 10, input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='tanh'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Conv1D(128, 3, activation='tanh'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(400, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(doc_terms_padded[is_training], y[is_training]\n",
    "          , validation_data=(doc_terms_padded[~is_training], y[~is_training])\n",
    "          , batch_size=64\n",
    "          , epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
